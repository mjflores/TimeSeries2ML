# TimeSeries to Machine Learning input

This program converts a univariate  time series into a format suitable for Machine Learning models by applying a sliding window approach, often referred to as lagging. Given a specified window size (lag), it extracts past observations to construct input features, aligning them with corresponding target values. This transformation is essential for supervised learning tasks involving temporal data, enabling models to capture patterns, dependencies, and trends over time for forecasting, classification, or anomaly detection applications across various domains.

